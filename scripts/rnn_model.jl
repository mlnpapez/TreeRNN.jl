#=
###############################################
#                                             #
#           RNN Model Implementation          #
#                    BOB                      #
#                                             #
###############################################

This file implements a Recurrent Neural Network (RNN) model
for sequence (next character) prediction based on the dataset generated by ALICE.

Author: Do Viet Anh
=#

using Flux
using LinearAlgebra
using Statistics

include("data_generator.jl")

# Define the RNN cell structure
struct RNNCell{T}
    w::Matrix{T}  # Input-to-hidden weights
    u::Matrix{T}  # Hidden-to-hidden weights
    b::Vector{T}  # Bias
end

Flux.@functor RNNCell

"""
Initialize an RNN cell with given input and hidden sizes
"""
function RNNCell(input_size::Int, hidden_size::Int; init=Flux.glorot_uniform)
    return RNNCell(
        init(hidden_size, input_size),
        init(hidden_size, hidden_size),
        init(hidden_size)
    )
end

"""
Forward pass for the RNN cell
"""
function (m::RNNCell)(h::AbstractVector, x::AbstractVector)
    return tanh.(m.w * x .+ m.u * h .+ m.b)
end

# Define the full RNN model structure
struct RNN
    cell::RNNCell
    output::Chain
end

Flux.@functor RNN

"""
Initialize the full RNN model
"""
function RNN(input_size::Int, hidden_size::Int, output_size::Int)
    RNN(
        RNNCell(input_size, hidden_size),
        Chain(Dense(hidden_size, output_size), softmax)
    )
end

"""
Forward pass for the full RNN model
"""
function (m::RNN)(x::AbstractMatrix)
    h = zeros(Float32, size(m.cell.u, 1))
    outputs = map(1:size(x,2)) do t
        h = m.cell(h, x[:, t])
        m.output(h)
    end
    return hcat(outputs...)
end

"""
Prepare sequence data for the RNN model
"""
function prepare_data(sequences)
    X = []
    Y = []
    for seq in sequences
        for i in 1:(length(seq)-1)
            push!(X, Flux.onehot(seq[i], VOCAB))
            push!(Y, Flux.onehot(seq[i+1], VOCAB))
        end
    end
    return hcat(X...), hcat(Y...)
end

"""
Train the RNN model
"""
function train_model!(model, X, Y, epochs)
    function loss(x, y)
        predictions = model(x)
        return Flux.crossentropy(predictions, y)
    end
    opt = ADAM()
    for epoch in 1:epochs
        Flux.train!(loss, Flux.params(model), [(X, Y)], opt)
        if epoch % 10 == 0
            println("loss(X, Y) =", loss(X, Y))
        end
    end
end

"""
Evaluate the RNN model's accuracy
"""
function evaluate_model(model, X, Y)
    predictions = model(X)
    accuracy = mean(Flux.onecold(predictions) .== Flux.onecold(Y))
    return accuracy
end

"""
Main function to run the entire RNN pipeline
"""
function main()
    # Generate dataset using ALICE
    initial_probs, transition_probs = generate_probabilities()
    dataset = generate_dataset(initial_probs, transition_probs, 140)

    # Prepare data for the model
    X, Y = prepare_data(dataset)
    
    println("X type: ", typeof(X))
    println("Y type: ", typeof(Y))
    println("X shape: ", size(X))
    println("Y shape: ", size(Y))

    # Split data into train and test sets
    split_idx = Int(floor(0.8 * size(X, 2)))
    X_train, Y_train = X[:, 1:split_idx], Y[:, 1:split_idx]
    X_test, Y_test = X[:, split_idx+1:end], Y[:, split_idx+1:end]

    # Create and train the model
    model = RNN(length(VOCAB), 32, length(VOCAB))
    
    # Print model output for a small batch
    small_batch = X[:, 1:5]
    println("Model output shape: ", size(model(small_batch)))

    # Print initial loss
    initial_loss = Flux.crossentropy(model(X), Y)
    println("Initial loss: ", initial_loss)

    # Train the model
    train_model!(model, X_train, Y_train, 100)

    # Evaluate the model
    train_accuracy = evaluate_model(model, X_train, Y_train)
    test_accuracy = evaluate_model(model, X_test, Y_test)

    println("Train accuracy: ", train_accuracy)
    println("Test accuracy: ", test_accuracy)
end

# Run the main function
main()